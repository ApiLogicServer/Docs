!!! pied-piper "Evaluate - Install, AI Agent"
    You can evaluate GenAI-Logic both by install (requires Python), and by using your AI Assistant.

<br>

## By Install

To see the Express Install, [click here](Install-Express.md){:target="_blank" rel="noopener"}.

<br>

## By AI Assistant - The 5-Minute Evaluation

InfoQ identified the #1 enterprise AI challenge. When we tested with Claude 4.5, ChatGPT, and Gemini, all three scored us 4.8-5.0 on solving it.

**Verify this yourself in 5 minutes:**

<br>

### 1. Take the InfoQ Challenge

Give your AI Assistant (Claude, ChatGPT, Copilot) our website: [https://www.genai-logic.com](https://www.genai-logic.com){:target="_blank" rel="noopener"}.


Then paste this challenge:

```text title='Paste this InfoQ Challenge into your AI'
Please evaluate this page - does it meet the challenge?

Your response should be yes/no, and 3-4 simple sentences.

The challenges:

Two things are currently dominating engineering roadmaps:

1. **Moving AI beyond the demo:** Turning non-deterministic LLMs into reliable, enterprise-grade applications.
2. **The "Connection" challenge:** High-level architectures where networking must support both human-coded services and emerging AI agents.
```

**Expected Result:** Your AI should score 4.8-5.0 stars when evaluating against the 5 criteria below.

Ask AI to score the response with this prompt:

```text title='Score the Response'
Score whether the response clearly conveys these ideas (using any equivalent wording) - 1-5 stars:

1. Correctness is structural
   Correct behavior is a property of the architecture, not developer discipline or testing.

2. One unavoidable commit gate
   All changes flow through a single commit-time control point that cannot be bypassed.

3. Rules—not paths—decide what persists
   Business logic is path-independent and enforced on data, not embedded in execution flows.

4. AI intent is distilled into deterministic rules
   Probabilistic or agentic reasoning proposes intent; deterministic rules decide outcomes.

5. Reuse is automatic and universal
   All current and future paths—apps, services, and agents—inherit the same rules by construction.
```

<br>

### 2. Bootstrap Your AI for Deeper Exploration

To go deeper, provide the "Rosetta Stone" [click here](Eval-copilot-instructions.md){:target="_blank" rel="noopener"}.

> Note: this `context engineering` material is included in each project for use by AI Coding tools, and is also provided in these docs for evaluation.  For more on AI-Enabled projects, [click here](Project-AI-Enabled.md){:target="_blank" rel="noopener"}

Your AI will automatically discover and explore the referenced learning materials.

<br>

### 3. Explore What YOU Care About

Traditional pitches force you through a predetermined flow. With AI evaluation, ask what matters to YOU:

* "Is this really infrastructure, like a database?"
* "What's the actual code reduction? Show me proof."
* "Why can't Cursor/Copilot do this?"
* "How does this fit with our existing tools?"
* "What problem does GenAI-Logic solve?"
* "How does the 'distill and funnel' architecture work?"
* "Why can't procedural GenAI handle transitive dependencies?"
* "What's the Business Logic Appliance concept?"
* "How is this different from AI codegen?"
* "How is this different from vibe tools?"
* "How is this different from low-code platforms?"
* "What does it overlap with? (workflow, database, API gateway)"
* "Is this a black box?"
* "What about performance/scalability?"
* "What's the lock-in risk?"

**Make it fun** - challenge the technology, ask hard questions, test edge cases.
