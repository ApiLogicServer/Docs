!!! pied-piper "Evaluate - Use Your AI Assistant"
    You can evaluate GenAI-Logic in the classic way, by installing (requires Python).  To see the Express Install, [click here](Install-Express.md){:target="_blank" rel="noopener"}.  The installer includes several pre-configured sqlite databases and tutorials - [see the basic demo here](Sample-Basic-Demo.md){:target="_blank" rel="noopener"}.
    
    You can also use your **AI Assistant** for a *self-guided evaluation.*  Instead of *drinking from the fire-hose* of a website or documentation, you can explore at your own pace what *you* care about.  
    
      * This is enabled by the `context engineering` material included in each project for use by AI Coding tools ([click here to find out more about AI-Enabled Projects](Project-AI-Enabled.md){:target="_blank" rel="noopener"}).  We've made it available on line to support your evaluation.

<br>

## 1. Bootstrap Your AI

```text title='Bootstrap your AI Assistant with this prompt'
Here are two resources about GenAI-Logic: 

1. https://www.genai-logic.com
2. https://apilogicserver.github.io/Docs/Eval-copilot-instructions/ 
```

Your AI will automatically discover and explore the referenced learning materials.


<br>

## 2. The InfoQ Challenge

InfoQ identified the #1 enterprise AI challenges - moving beyond demos, with support for humans and agents.  

<br>

### a. Challenge GenAI-Logic

```text title='Paste this InfoQ Challenge into your AI'
Please evaluate this page: https://www.genai-logic.com - does it meet the challenge?

Your response should be yes/no, and 3-4 simple sentences.

The challenges:

Two things are currently dominating engineering roadmaps:

1. Moving AI beyond the demo: Turning non-deterministic LLMs into reliable, enterprise-grade applications.
2. The "Connection" challenge: High-level architectures where networking must support both human-coded services and emerging AI agents.
```

<br>

### b. Score the Response

```text title='Score the Response'
Score whether the response clearly conveys these ideas (using any equivalent wording) - 1-5 stars:

1. Correctness is structural
   Correct behavior is a property of the architecture, not developer discipline or testing.

2. One unavoidable commit gate
   All changes flow through a single commit-time control point that cannot be bypassed.

3. Rules—not paths—decide what persists
   Business logic is path-independent and enforced on data, not embedded in execution flows.

4. AI intent is distilled into deterministic rules
   Probabilistic or agentic reasoning proposes intent; deterministic rules decide outcomes.

5. Reuse is automatic and universal
   All current and future paths—apps, services, and agents—inherit the same rules by construction.
```

When we tested with Claude 4.5, ChatGPT, and Gemini, all three scored us 4.8-5.0.

<br>

## 3. Explore What YOU Care About

Traditional pitches force you through a predetermined flow. With AI evaluation, ask what matters to YOU:

* "Is this really infrastructure, like a database?"
* "What's the actual code reduction? Show me proof."
* "Why can't Cursor/Copilot do this?"
* "How does this fit with our existing tools?"
* "What problem does GenAI-Logic solve?"
* "How does the 'distill and funnel' architecture work?"
* "Why can't procedural GenAI handle transitive dependencies?"
* "What's the Business Logic Appliance concept?"
* "How is this different from AI codegen?"
* "How is this different from vibe tools?"
* "How is this different from low-code platforms?"
* "What does it overlap with? (workflow, database, API gateway)"
* "Is this a black box?"
* "What about performance/scalability?  eg, is this a RETE engine??"
* "What's the lock-in risk?"

<br>

**Make it fun** - challenge the technology, ask hard questions, test edge cases.
